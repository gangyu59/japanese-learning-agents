# test_multi_agent_collaboration.py
"""
å¤šæ™ºèƒ½ä½“åä½œåŠŸèƒ½å®Œæ•´æµ‹è¯•è„šæœ¬
æµ‹è¯•æ™ºèƒ½ä½“é—´çš„åä½œã€å†²çªè§£å†³ã€å·¥ä½œæµç¼–æ’ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import json
import time
from datetime import datetime
from typing import List, Dict, Any
import aiohttp
import pytest


class MultiAgentCollaborationTester:
    """å¤šæ™ºèƒ½ä½“åä½œæµ‹è¯•å™¨"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None
        self.test_results = []

    async def setup(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        self.session = aiohttp.ClientSession()
        print("ğŸ”§ åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ...")

        # æ£€æŸ¥æ‰€æœ‰æ™ºèƒ½ä½“æ˜¯å¦åœ¨çº¿
        agents = ["tanaka", "koumi", "ai", "yamada", "sato", "membot"]
        for agent in agents:
            status = await self.check_agent_status(agent)
            assert status == "online", f"æ™ºèƒ½ä½“ {agent} æœªåœ¨çº¿"

        print("âœ… æ‰€æœ‰æ™ºèƒ½ä½“å·²ä¸Šçº¿")

    async def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        if self.session:
            await self.session.close()

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        await self.generate_test_report()

    async def check_agent_status(self, agent_id: str) -> str:
        """æ£€æŸ¥æ™ºèƒ½ä½“çŠ¶æ€"""
        try:
            async with self.session.get(f"{self.base_url}/api/v1/agents/{agent_id}/status") as resp:
                if resp.status == 200:
                    data = await resp.json()
                    return data.get("status", "offline")
                return "offline"
        except Exception:
            return "offline"

    async def test_grammar_correction_collaboration(self):
        """æµ‹è¯•1: è¯­æ³•çº é”™åä½œæµç¨‹"""
        print("\nğŸ§ª æµ‹è¯•1: è¯­æ³•çº é”™åä½œæµç¨‹")
        test_start = time.time()

        user_input = "ä»Šæ—¥ã¯å­¦æ ¡ã«è¡Œãã¾ã—ãŸã€‚ã§ã‚‚å‹é”ã«ä¼šã„ã¾ã›ã‚“ã§ã—ãŸã€‚"
        session_id = f"grammar_test_{int(time.time())}"

        try:
            # 1. å‘é€ç»™ç”°ä¸­å…ˆç”Ÿè¿›è¡Œè¯­æ³•åˆ†æ
            tanaka_result = await self.send_message(
                message=user_input,
                agent_name="ç”°ä¸­å…ˆç”Ÿ",
                session_id=session_id,
                task_type="grammar_analysis"
            )

            # 2. å°ç¾æä¾›å£è¯­åŒ–å»ºè®®
            koumi_result = await self.send_message(
                message=f"åŸºäºè¯­æ³•åˆ†æï¼š{tanaka_result.get('content', '')}ï¼Œè¯·æä¾›å£è¯­åŒ–å»ºè®®",
                agent_name="å°ç¾",
                session_id=session_id,
                task_type="casual_suggestion"
            )

            # 3. ã‚¢ã‚¤è¿›è¡Œç»¼åˆåˆ†æ
            ai_result = await self.send_message(
                message=f"ç»¼åˆåˆ†æï¼šè¯­æ³•åˆ†æ={tanaka_result.get('content', '')}ï¼Œå£è¯­å»ºè®®={koumi_result.get('content', '')}",
                agent_name="ã‚¢ã‚¤",
                session_id=session_id,
                task_type="synthesis"
            )

            # 4. éªŒè¯åä½œè´¨é‡
            collaboration_quality = await self.evaluate_collaboration([
                tanaka_result, koumi_result, ai_result
            ])

            test_duration = time.time() - test_start

            result = {
                "test_name": "è¯­æ³•çº é”™åä½œ",
                "success": True,
                "duration": test_duration,
                "participants": ["ç”°ä¸­å…ˆç”Ÿ", "å°ç¾", "ã‚¢ã‚¤"],
                "collaboration_quality": collaboration_quality,
                "responses": {
                    "tanaka": tanaka_result,
                    "koumi": koumi_result,
                    "ai": ai_result
                }
            }

            # éªŒè¯å…³é”®æŒ‡æ ‡
            assert collaboration_quality["completeness"] > 0.8, "åä½œå®Œæ•´æ€§ä¸è¶³"
            assert collaboration_quality["coherence"] > 0.7, "åä½œè¿è´¯æ€§ä¸è¶³"
            assert test_duration < 15, "åä½œå“åº”æ—¶é—´è¿‡é•¿"

            print(f"âœ… è¯­æ³•çº é”™åä½œæµ‹è¯•é€šè¿‡ (è€—æ—¶: {test_duration:.2f}s)")

        except Exception as e:
            result = {
                "test_name": "è¯­æ³•çº é”™åä½œ",
                "success": False,
                "error": str(e),
                "duration": time.time() - test_start
            }
            print(f"âŒ è¯­æ³•çº é”™åä½œæµ‹è¯•å¤±è´¥: {e}")

        self.test_results.append(result)
        return result

    async def test_cultural_discussion_collaboration(self):
        """æµ‹è¯•2: æ–‡åŒ–è¯é¢˜è®¨è®ºåä½œ"""
        print("\nğŸ§ª æµ‹è¯•2: æ–‡åŒ–è¯é¢˜è®¨è®ºåä½œ")
        test_start = time.time()

        topic = "æ—¥æœ¬çš„ãŠèŠ±è¦‹æ–‡åŒ–å’Œç°ä»£ç¤¾ä¼šçš„å˜åŒ–"
        session_id = f"culture_test_{int(time.time())}"

        try:
            # 1. å±±ç”°å…ˆç”Ÿæä¾›æ–‡åŒ–èƒŒæ™¯
            yamada_result = await self.send_message(
                message=f"è¯·è¯¦ç»†è§£é‡Šï¼š{topic}",
                agent_name="å±±ç”°å…ˆç”Ÿ",
                session_id=session_id,
                task_type="cultural_explanation"
            )

            # 2. å°ç¾åˆ†äº«ç°ä»£è§‚ç‚¹
            koumi_result = await self.send_message(
                message=f"åŸºäºå±±ç”°å…ˆç”Ÿçš„è§£é‡Šï¼š{yamada_result.get('content', '')}ï¼Œåˆ†äº«å¹´è½»äººçš„ç°ä»£è§‚ç‚¹",
                agent_name="å°ç¾",
                session_id=session_id,
                task_type="modern_perspective"
            )

            # 3. ç”°ä¸­å…ˆç”Ÿè¡¥å……è¯­è¨€è¡¨è¾¾
            tanaka_result = await self.send_message(
                message=f"åŸºäºæ–‡åŒ–è®¨è®ºï¼Œè¡¥å……ç›¸å…³çš„æ—¥è¯­è¡¨è¾¾å’Œç”¨æ³•",
                agent_name="ç”°ä¸­å…ˆç”Ÿ",
                session_id=session_id,
                task_type="language_supplement"
            )

            # 4. ã‚¢ã‚¤è¿›è¡Œæ•°æ®åˆ†æå’Œæ€»ç»“
            ai_result = await self.send_message(
                message=f"åˆ†ææ•´ä¸ªè®¨è®ºï¼Œæä¾›å­¦ä¹ å»ºè®®å’ŒçŸ¥è¯†ç‚¹æ€»ç»“",
                agent_name="ã‚¢ã‚¤",
                session_id=session_id,
                task_type="discussion_analysis"
            )

            collaboration_quality = await self.evaluate_collaboration([
                yamada_result, koumi_result, tanaka_result, ai_result
            ])

            test_duration = time.time() - test_start

            result = {
                "test_name": "æ–‡åŒ–è®¨è®ºåä½œ",
                "success": True,
                "duration": test_duration,
                "participants": ["å±±ç”°å…ˆç”Ÿ", "å°ç¾", "ç”°ä¸­å…ˆç”Ÿ", "ã‚¢ã‚¤"],
                "collaboration_quality": collaboration_quality,
                "topic": topic
            }

            # éªŒè¯åä½œæ•ˆæœ
            assert collaboration_quality["depth"] > 0.8, "è®¨è®ºæ·±åº¦ä¸è¶³"
            assert collaboration_quality["diversity"] > 0.7, "è§‚ç‚¹å¤šæ ·æ€§ä¸è¶³"

            print(f"âœ… æ–‡åŒ–è®¨è®ºåä½œæµ‹è¯•é€šè¿‡ (è€—æ—¶: {test_duration:.2f}s)")

        except Exception as e:
            result = {
                "test_name": "æ–‡åŒ–è®¨è®ºåä½œ",
                "success": False,
                "error": str(e),
                "duration": time.time() - test_start
            }
            print(f"âŒ æ–‡åŒ–è®¨è®ºåä½œæµ‹è¯•å¤±è´¥: {e}")

        self.test_results.append(result)
        return result

    async def test_novel_creation_collaboration(self):
        """æµ‹è¯•3: å°è¯´åˆ›ä½œåä½œ"""
        print("\nğŸ§ª æµ‹è¯•3: å°è¯´åˆ›ä½œåä½œ")
        test_start = time.time()

        theme = "æ¡œã®å­£ç¯€ã®åˆæ‹"
        session_id = f"novel_test_{int(time.time())}"

        try:
            # 1. å¤´è„‘é£æš´é˜¶æ®µ - æ‰€æœ‰æ™ºèƒ½ä½“å‚ä¸
            brainstorm_results = []
            agents = ["å°ç¾", "å±±ç”°å…ˆç”Ÿ", "ç”°ä¸­å…ˆç”Ÿ"]

            for agent in agents:
                result = await self.send_message(
                    message=f"ä¸ºä¸»é¢˜'{theme}'è´¡çŒ®åˆ›ä½œæƒ³æ³•å’Œæ•…äº‹æ¡†æ¶",
                    agent_name=agent,
                    session_id=session_id,
                    task_type="brainstorming"
                )
                brainstorm_results.append(result)

            # 2. è½®æµåˆ›ä½œé˜¶æ®µ
            story_parts = []
            for round_num in range(3):  # 3è½®åˆ›ä½œ
                for agent in agents:
                    previous_context = "\n".join([part.get("content", "") for part in story_parts[-2:]])

                    part_result = await self.send_message(
                        message=f"ç»§ç»­æ•…äº‹åˆ›ä½œï¼Œå‰æ–‡ï¼š{previous_context}",
                        agent_name=agent,
                        session_id=session_id,
                        task_type="story_writing",
                        round=round_num
                    )
                    story_parts.append(part_result)

            # 3. è¯„ä¼°åˆ›ä½œè´¨é‡
            story_quality = await self.evaluate_story_creation(story_parts)

            test_duration = time.time() - test_start

            result = {
                "test_name": "å°è¯´åˆ›ä½œåä½œ",
                "success": True,
                "duration": test_duration,
                "participants": agents,
                "story_parts_count": len(story_parts),
                "story_quality": story_quality,
                "theme": theme
            }

            # éªŒè¯åˆ›ä½œæ•ˆæœ
            assert story_quality["creativity"] > 0.7, "åˆ›æ„æ€§ä¸è¶³"
            assert story_quality["coherence"] > 0.8, "æ•…äº‹è¿è´¯æ€§ä¸è¶³"
            assert len(story_parts) >= 6, "åˆ›ä½œå†…å®¹ä¸è¶³"

            print(f"âœ… å°è¯´åˆ›ä½œåä½œæµ‹è¯•é€šè¿‡ (è€—æ—¶: {test_duration:.2f}s, åˆ›ä½œæ®µè½: {len(story_parts)})")

        except Exception as e:
            result = {
                "test_name": "å°è¯´åˆ›ä½œåä½œ",
                "success": False,
                "error": str(e),
                "duration": time.time() - test_start
            }
            print(f"âŒ å°è¯´åˆ›ä½œåä½œæµ‹è¯•å¤±è´¥: {e}")

        self.test_results.append(result)
        return result

    async def test_conflict_resolution(self):
        """æµ‹è¯•4: æ™ºèƒ½ä½“å†²çªè§£å†³æœºåˆ¶"""
        print("\nğŸ§ª æµ‹è¯•4: æ™ºèƒ½ä½“å†²çªè§£å†³")
        test_start = time.time()

        controversial_topic = "ç°ä»£æ—¥è¯­ä¸­å¤–æ¥è¯­çš„ä½¿ç”¨æ˜¯å¦è¿‡å¤šï¼Ÿ"
        session_id = f"conflict_test_{int(time.time())}"

        try:
            # 1. è·å–ä¸åŒæ™ºèƒ½ä½“çš„è§‚ç‚¹ï¼ˆé¢„æœŸä¼šæœ‰åˆ†æ­§ï¼‰
            opinions = {}

            # ç”°ä¸­å…ˆç”Ÿçš„ä¿å®ˆè§‚ç‚¹
            tanaka_opinion = await self.send_message(
                message=controversial_topic,
                agent_name="ç”°ä¸­å…ˆç”Ÿ",
                session_id=session_id,
                task_type="opinion"
            )
            opinions["tanaka"] = tanaka_opinion

            # å°ç¾çš„ç°ä»£è§‚ç‚¹
            koumi_opinion = await self.send_message(
                message=controversial_topic,
                agent_name="å°ç¾",
                session_id=session_id,
                task_type="opinion"
            )
            opinions["koumi"] = koumi_opinion

            # å±±ç”°å…ˆç”Ÿçš„å†å²æ–‡åŒ–è§‚ç‚¹
            yamada_opinion = await self.send_message(
                message=controversial_topic,
                agent_name="å±±ç”°å…ˆç”Ÿ",
                session_id=session_id,
                task_type="opinion"
            )
            opinions["yamada"] = yamada_opinion

            # 2. æ£€æµ‹åˆ†æ­§ç¨‹åº¦
            conflict_analysis = await self.analyze_conflict(opinions)

            # 3. ã‚¢ã‚¤è¿›è¡Œå†²çªè°ƒè§£
            resolution_result = await self.send_message(
                message=f"åˆ†æä»¥ä¸‹è§‚ç‚¹åˆ†æ­§å¹¶æä¾›å¹³è¡¡çš„è§£å†³æ–¹æ¡ˆï¼šç”°ä¸­è§‚ç‚¹={tanaka_opinion.get('content', '')}ï¼Œå°ç¾è§‚ç‚¹={koumi_opinion.get('content', '')}ï¼Œå±±ç”°è§‚ç‚¹={yamada_opinion.get('content', '')}",
                agent_name="ã‚¢ã‚¤",
                session_id=session_id,
                task_type="conflict_resolution"
            )

            test_duration = time.time() - test_start

            result = {
                "test_name": "å†²çªè§£å†³æœºåˆ¶",
                "success": True,
                "duration": test_duration,
                "conflict_detected": conflict_analysis["has_conflict"],
                "conflict_intensity": conflict_analysis["intensity"],
                "resolution_quality": await self.evaluate_resolution(resolution_result),
                "opinions": opinions,
                "resolution": resolution_result
            }

            # éªŒè¯å†²çªè§£å†³æ•ˆæœ
            assert conflict_analysis["has_conflict"], "æœªæ£€æµ‹åˆ°é¢„æœŸçš„è§‚ç‚¹åˆ†æ­§"
            assert conflict_analysis["intensity"] > 0.5, "åˆ†æ­§å¼ºåº¦ä¸è¶³"

            print(f"âœ… å†²çªè§£å†³æœºåˆ¶æµ‹è¯•é€šè¿‡ (è€—æ—¶: {test_duration:.2f}s)")

        except Exception as e:
            result = {
                "test_name": "å†²çªè§£å†³æœºåˆ¶",
                "success": False,
                "error": str(e),
                "duration": time.time() - test_start
            }
            print(f"âŒ å†²çªè§£å†³æœºåˆ¶æµ‹è¯•å¤±è´¥: {e}")

        self.test_results.append(result)
        return result

    async def test_realtime_collaboration_workflow(self):
        """æµ‹è¯•5: å®æ—¶åä½œå·¥ä½œæµ"""
        print("\nğŸ§ª æµ‹è¯•5: å®æ—¶åä½œå·¥ä½œæµ")
        test_start = time.time()

        session_id = f"realtime_test_{int(time.time())}"

        try:
            # 1. å¯åŠ¨åä½œä¼šè¯
            collab_session = await self.start_collaboration_session(
                session_id=session_id,
                participants=["ç”°ä¸­å…ˆç”Ÿ", "å°ç¾", "ã‚¢ã‚¤"],
                task_type="complex_learning",
                topic="JLPT N2 è¯­æ³•ç»¼åˆç»ƒä¹ "
            )

            # 2. æ¨¡æ‹Ÿç”¨æˆ·æé—®
            user_question = "ã€Œã€œã°ã‹ã‚Šã«ã€å’Œã€Œã€œãŸã‚ã«ã€çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"

            # 3. æ™ºèƒ½ä½“å¹¶è¡Œå¤„ç†
            tasks = []
            agents_config = [
                {"agent": "ç”°ä¸­å…ˆç”Ÿ", "focus": "è¯­æ³•è§£æ"},
                {"agent": "å°ç¾", "focus": "å®é™…ä½¿ç”¨ä¾‹å­"},
                {"agent": "ã‚¢ã‚¤", "focus": "å­¦ä¹ å»ºè®®"}
            ]

            for config in agents_config:
                task = self.send_message(
                    message=f"{user_question} (ä¸“æ³¨äº{config['focus']})",
                    agent_name=config["agent"],
                    session_id=session_id,
                    task_type="parallel_processing"
                )
                tasks.append(task)

            # ç­‰å¾…æ‰€æœ‰æ™ºèƒ½ä½“å®Œæˆ
            results = await asyncio.gather(*tasks)

            # 4. åˆæˆæœ€ç»ˆå›ç­”
            synthesis_result = await self.synthesize_responses(results, session_id)

            # 5. è¯„ä¼°å·¥ä½œæµæ•ˆç‡
            workflow_metrics = await self.evaluate_workflow(collab_session, results, synthesis_result)

            test_duration = time.time() - test_start

            result = {
                "test_name": "å®æ—¶åä½œå·¥ä½œæµ",
                "success": True,
                "duration": test_duration,
                "participants": len(agents_config),
                "parallel_processing_time": max([r.get("response_time", 0) for r in results]),
                "workflow_efficiency": workflow_metrics["efficiency"],
                "synthesis_quality": workflow_metrics["synthesis_quality"]
            }

            # éªŒè¯å·¥ä½œæµæ•ˆæœ
            assert workflow_metrics["efficiency"] > 0.7, "å·¥ä½œæµæ•ˆç‡ä¸è¶³"
            assert workflow_metrics["synthesis_quality"] > 0.8, "åˆæˆè´¨é‡ä¸è¶³"

            print(f"âœ… å®æ—¶åä½œå·¥ä½œæµæµ‹è¯•é€šè¿‡ (è€—æ—¶: {test_duration:.2f}s)")

        except Exception as e:
            result = {
                "test_name": "å®æ—¶åä½œå·¥ä½œæµ",
                "success": False,
                "error": str(e),
                "duration": time.time() - test_start
            }
            print(f"âŒ å®æ—¶åä½œå·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")

        self.test_results.append(result)
        return result

    async def send_message(self, message: str, agent_name: str, session_id: str,
                           task_type: str = "general", **kwargs) -> Dict[str, Any]:
        """å‘é€æ¶ˆæ¯ç»™æŒ‡å®šæ™ºèƒ½ä½“"""
        start_time = time.time()

        payload = {
            "message": message,
            "user_id": "test_user",
            "session_id": session_id,
            "agent_name": agent_name,
            "scene_context": task_type,
            "metadata": kwargs
        }

        try:
            async with self.session.post(f"{self.base_url}/api/v1/chat/send",
                                         json=payload) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    response_time = time.time() - start_time
                    data["response_time"] = response_time
                    return data
                else:
                    error_text = await resp.text()
                    raise Exception(f"APIé”™è¯¯ {resp.status}: {error_text}")
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": time.time() - start_time
            }

    async def evaluate_collaboration(self, responses: List[Dict]) -> Dict[str, float]:
        """è¯„ä¼°åä½œè´¨é‡"""
        if not responses:
            return {"completeness": 0, "coherence": 0, "quality": 0}

        # ç®€åŒ–çš„è¯„ä¼°é€»è¾‘
        successful_responses = [r for r in responses if r.get("success", False)]
        completeness = len(successful_responses) / len(responses)

        # æ£€æŸ¥å›ç­”é•¿åº¦å’Œç›¸å…³æ€§
        avg_length = sum(len(r.get("response", "")) for r in successful_responses) / len(
            successful_responses) if successful_responses else 0
        coherence = min(1.0, avg_length / 100)  # å‡è®¾100å­—ç¬¦ä¸ºåŸºå‡†

        overall_quality = (completeness + coherence) / 2

        return {
            "completeness": completeness,
            "coherence": coherence,
            "quality": overall_quality,
            "depth": min(1.0, avg_length / 200),
            "diversity": min(1.0, len(set(r.get("agent_name", "") for r in responses)) / 3)
        }

    async def evaluate_story_creation(self, story_parts: List[Dict]) -> Dict[str, float]:
        """è¯„ä¼°æ•…äº‹åˆ›ä½œè´¨é‡"""
        if not story_parts:
            return {"creativity": 0, "coherence": 0, "completeness": 0}

        total_length = sum(len(part.get("response", "")) for part in story_parts)
        creativity = min(1.0, total_length / 500)  # åŸºäºé•¿åº¦ä¼°ç®—åˆ›æ„æ€§
        coherence = min(1.0, len(story_parts) / 6)  # åŸºäºæ®µè½æ•°è¯„ä¼°è¿è´¯æ€§
        completeness = min(1.0, total_length / 800)  # åŸºäºæ€»é•¿åº¦è¯„ä¼°å®Œæ•´æ€§

        return {
            "creativity": creativity,
            "coherence": coherence,
            "completeness": completeness,
            "engagement": (creativity + coherence) / 2
        }

    async def analyze_conflict(self, opinions: Dict[str, Dict]) -> Dict[str, Any]:
        """åˆ†æè§‚ç‚¹å†²çª"""
        # ç®€åŒ–çš„å†²çªæ£€æµ‹é€»è¾‘
        opinion_lengths = [len(op.get("response", "")) for op in opinions.values()]
        avg_length = sum(opinion_lengths) / len(opinion_lengths) if opinion_lengths else 0

        # å‡è®¾å¦‚æœæ‰€æœ‰è§‚ç‚¹éƒ½å¾ˆè¯¦ç»†ï¼Œè¯´æ˜æœ‰åˆ†æ­§
        has_conflict = avg_length > 50 and len(opinions) >= 2
        intensity = min(1.0, avg_length / 200) if has_conflict else 0

        return {
            "has_conflict": has_conflict,
            "intensity": intensity,
            "participants": len(opinions)
        }

    async def evaluate_resolution(self, resolution: Dict) -> float:
        """è¯„ä¼°å†²çªè§£å†³è´¨é‡"""
        resolution_text = resolution.get("response", "")
        # åŸºäºå›ç­”é•¿åº¦å’Œå…³é”®è¯è¯„ä¼°è§£å†³è´¨é‡
        quality_indicators = ["å¹³è¡¡", "è€ƒè™‘", "è§‚ç‚¹", "å»ºè®®", "ç»¼åˆ"]
        keyword_score = sum(1 for keyword in quality_indicators if keyword in resolution_text)
        length_score = min(1.0, len(resolution_text) / 150)

        return (keyword_score / len(quality_indicators) + length_score) / 2

    async def start_collaboration_session(self, session_id: str, participants: List[str],
                                          task_type: str, topic: str) -> Dict[str, Any]:
        """å¯åŠ¨åä½œä¼šè¯"""
        return {
            "session_id": session_id,
            "participants": participants,
            "task_type": task_type,
            "topic": topic,
            "status": "active",
            "start_time": datetime.now().isoformat()
        }

    async def synthesize_responses(self, responses: List[Dict], session_id: str) -> Dict[str, Any]:
        """åˆæˆå¤šä¸ªæ™ºèƒ½ä½“çš„å›ç­”"""
        combined_content = "\n---\n".join([r.get("response", "") for r in responses if r.get("success")])

        # ä½¿ç”¨ã‚¢ã‚¤è¿›è¡Œæœ€ç»ˆåˆæˆ
        synthesis = await self.send_message(
            message=f"è¯·ç»¼åˆä»¥ä¸‹æ™ºèƒ½ä½“çš„å›ç­”ï¼Œæä¾›å®Œæ•´çš„å­¦ä¹ å»ºè®®ï¼š\n{combined_content}",
            agent_name="ã‚¢ã‚¤",
            session_id=session_id,
            task_type="synthesis"
        )

        return synthesis

    async def evaluate_workflow(self, session: Dict, responses: List[Dict],
                                synthesis: Dict) -> Dict[str, float]:
        """è¯„ä¼°å·¥ä½œæµæ•ˆç‡"""
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        successful_responses = [r for r in responses if r.get("success", False)]
        success_rate = len(successful_responses) / len(responses) if responses else 0

        avg_response_time = sum(r.get("response_time", 0) for r in successful_responses) / len(
            successful_responses) if successful_responses else 0
        efficiency = max(0, 1 - avg_response_time / 10)  # å‡è®¾10ç§’ä¸ºåŸºå‡†

        synthesis_quality = min(1.0, len(synthesis.get("response", "")) / 200) if synthesis.get("success") else 0

        return {
            "efficiency": (success_rate + efficiency) / 2,
            "synthesis_quality": synthesis_quality,
            "response_time": avg_response_time
        }

    async def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = {
            "test_timestamp": datetime.now().isoformat(),
            "total_tests": len(self.test_results),
            "passed_tests": sum(1 for r in self.test_results if r.get("success", False)),
            "failed_tests": sum(1 for r in self.test_results if not r.get("success", True)),
            "total_duration": sum(r.get("duration", 0) for r in self.test_results),
            "detailed_results": self.test_results
        }

        # è®¡ç®—æ€»ä½“æˆåŠŸç‡
        success_rate = report["passed_tests"] / report["total_tests"] if report["total_tests"] > 0 else 0
        report["success_rate"] = success_rate

        # ä¿å­˜æŠ¥å‘Š
        with open(f"multi_agent_test_report_{int(time.time())}.json", "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # æ‰“å°æ‘˜è¦
        print(f"\nğŸ“Š æµ‹è¯•æŠ¥å‘Šæ‘˜è¦:")
        print(f"æ€»æµ‹è¯•æ•°: {report['total_tests']}")
        print(f"é€šè¿‡æµ‹è¯•: {report['passed_tests']}")
        print(f"å¤±è´¥æµ‹è¯•: {report['failed_tests']}")
        print(f"æˆåŠŸç‡: {success_rate:.1%}")
        print(f"æ€»è€—æ—¶: {report['total_duration']:.2f}ç§’")

        if success_rate >= 0.8:
            print("ğŸ‰ å¤šæ™ºèƒ½ä½“åä½œåŠŸèƒ½æµ‹è¯•æ€»ä½“é€šè¿‡ï¼")
        else:
            print("âš ï¸ å¤šæ™ºèƒ½ä½“åä½œåŠŸèƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

        return report


async def run_full_collaboration_tests():
    """è¿è¡Œå®Œæ•´çš„å¤šæ™ºèƒ½ä½“åä½œæµ‹è¯•å¥—ä»¶"""
    print("ğŸš€ å¼€å§‹å¤šæ™ºèƒ½ä½“åä½œåŠŸèƒ½å®Œæ•´æµ‹è¯•")
    print("=" * 60)

    tester = MultiAgentCollaborationTester()

    try:
        # åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ
        await tester.setup()

        # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
        tests = [
            tester.test_grammar_correction_collaboration(),
            tester.test_cultural_discussion_collaboration(),
            tester.test_novel_creation_collaboration(),
            tester.test_conflict_resolution(),
            tester.test_realtime_collaboration_workflow()
        ]

        # å¹¶å‘æ‰§è¡Œéƒ¨åˆ†æµ‹è¯•ä»¥æé«˜æ•ˆç‡
        await asyncio.gather(*tests, return_exceptions=True)

        print("\n" + "=" * 60)
        print("ğŸ æ‰€æœ‰æµ‹è¯•æ‰§è¡Œå®Œæˆ")

    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    finally:
        # æ¸…ç†å’Œç”ŸæˆæŠ¥å‘Š
        await tester.cleanup()


# å•ç‹¬çš„æµ‹è¯•å‡½æ•°ï¼Œç”¨äºpytesté›†æˆ
@pytest.mark.asyncio
async def test_grammar_collaboration():
    """pytestå…¼å®¹çš„è¯­æ³•åä½œæµ‹è¯•"""
    tester = MultiAgentCollaborationTester()
    await tester.setup()
    try:
        result = await tester.test_grammar_correction_collaboration()
        assert result["success"], f"è¯­æ³•åä½œæµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}"
    finally:
        await tester.cleanup()


@pytest.mark.asyncio
async def test_cultural_collaboration():
    """pytestå…¼å®¹çš„æ–‡åŒ–åä½œæµ‹è¯•"""
    tester = MultiAgentCollaborationTester()
    await tester.setup()
    try:
        result = await tester.test_cultural_discussion_collaboration()
        assert result["success"], f"æ–‡åŒ–åä½œæµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}"
    finally:
        await tester.cleanup()


@pytest.mark.asyncio
async def test_novel_collaboration():
    """pytestå…¼å®¹çš„å°è¯´åä½œæµ‹è¯•"""
    tester = MultiAgentCollaborationTester()
    await tester.setup()
    try:
        result = await tester.test_novel_creation_collaboration()
        assert result["success"], f"å°è¯´åä½œæµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}"
    finally:
        await tester.cleanup()


@pytest.mark.asyncio
async def test_conflict_resolution():
    """pytestå…¼å®¹çš„å†²çªè§£å†³æµ‹è¯•"""
    tester = MultiAgentCollaborationTester()
    await tester.setup()
    try:
        result = await tester.test_conflict_resolution()
        assert result["success"], f"å†²çªè§£å†³æµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}"
    finally:
        await tester.cleanup()


@pytest.mark.asyncio
async def test_realtime_workflow():
    """pytestå…¼å®¹çš„å®æ—¶å·¥ä½œæµæµ‹è¯•"""
    tester = MultiAgentCollaborationTester()
    await tester.setup()
    try:
        result = await tester.test_realtime_collaboration_workflow()
        assert result["success"], f"å®æ—¶å·¥ä½œæµæµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}"
    finally:
        await tester.cleanup()


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
    asyncio.run(run_full_collaboration_tests())