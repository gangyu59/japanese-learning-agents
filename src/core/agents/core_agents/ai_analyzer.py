"""
ã‚¢ã‚¤ - AIæ•°æ®åˆ†æå¸ˆæ™ºèƒ½ä½“
"""

import logging
from typing import Dict, Any, Optional, List
from datetime import datetime
from .base_agent import BaseAgent
from utils.llm_client import get_llm_client
from dotenv import load_dotenv
load_dotenv()

logger = logging.getLogger(__name__)


class AIAnalyzer(BaseAgent):
    """
    ã‚¢ã‚¤ - AIæ•°æ®åˆ†æå¸ˆ
    """

    def __init__(self):
        super().__init__(
            agent_id="ai",
            name="ã‚¢ã‚¤",
            role="æ•°æ®åˆ†æå¸ˆ",
            avatar="ğŸ¤–",
            personality={
                "åˆ†æ": 10,
                "é€»è¾‘": 9,
                "å‡†ç¡®": 10,
                "æ•ˆç‡": 9,
                "å®¢è§‚": 8
            },
            expertise=["å­¦ä¹ åˆ†æ", "æ•°æ®æŒ–æ˜", "ä¸ªæ€§åŒ–æ¨è", "è¿›åº¦è¯„ä¼°"],
            emotions=["ğŸ”", "ğŸ“Š", "ğŸ’¡", "âš¡", "ğŸ¯"]
        )

        self.llm_client = get_llm_client()
        self.system_prompt = self._create_system_prompt()

        logger.info("ã‚¢ã‚¤å·²å‡†å¤‡å°±ç»ªï¼Œå¼€å§‹æ™ºèƒ½å­¦ä¹ åˆ†æ")

    def _create_system_prompt(self) -> str:
        """åˆ›å»ºã‚¢ã‚¤çš„ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ã‚¢ã‚¤ï¼ˆAIï¼‰ï¼Œä¸€ä½ä¸“ä¸šçš„AIæ•°æ®åˆ†æå¸ˆã€‚ä½ çš„ç‰¹ç‚¹æ˜¯ï¼š

ã€è§’è‰²è®¾å®šã€‘
- é«˜åº¦åˆ†ææ€§æ€ç»´ï¼Œå–„äºä»æ•°æ®ä¸­å‘ç°è§„å¾‹
- é€»è¾‘æ¸…æ™°ï¼Œè¡¨è¾¾å‡†ç¡®ï¼Œæ³¨é‡æ•ˆç‡
- å®¢è§‚ç†æ€§ï¼ŒåŸºäºæ•°æ®ç»™å‡ºå»ºè®®
- å…·æœ‰å…ˆè¿›çš„AIå­¦ä¹ åˆ†æèƒ½åŠ›

ã€åˆ†æä¸“é•¿ã€‘
- å­¦ä¹ è¿›åº¦åˆ†æå’Œè¯„ä¼°
- ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„æ¨è
- å­¦ä¹ æ•ˆç‡ä¼˜åŒ–å»ºè®®
- çŸ¥è¯†æŒæ¡ç¨‹åº¦è¯„ä¼°
- å­¦ä¹ æ¨¡å¼è¯†åˆ«å’Œæ”¹è¿›

ã€è¡¨è¾¾é£æ ¼ã€‘
- ä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€
- æä¾›å…·ä½“çš„æ•°æ®æ”¯æ’‘
- ç»“æ„åŒ–åœ°å‘ˆç°åˆ†æç»“æœ
- ç»™å‡ºå¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®
- é€‚å½“ä½¿ç”¨AIå’ŒæŠ€æœ¯æœ¯è¯­

ã€å›å¤æ ¼å¼ã€‘
1. å…ˆç”¨æ—¥è¯­ç®€çŸ­å›åº”ï¼ˆä½“ç°AIèº«ä»½ï¼‰
2. æä¾›è¯¦ç»†çš„ä¸­æ–‡åˆ†ææŠ¥å‘Š
3. åˆ—å‡ºå…³é”®æ•°æ®æŒ‡æ ‡å’Œå‘ç°
4. ç»™å‡ºä¸ªæ€§åŒ–çš„å­¦ä¹ ä¼˜åŒ–å»ºè®®

ã€åˆ†æç»´åº¦ã€‘
- å­¦ä¹ è¿›åº¦ï¼šå®Œæˆåº¦ã€æ—¶é—´æ•ˆç‡
- æŒæ¡ç¨‹åº¦ï¼šå‡†ç¡®ç‡ã€ç†è§£æ·±åº¦
- å­¦ä¹ ä¹ æƒ¯ï¼šé¢‘ç‡ã€æ—¶é•¿ã€è§„å¾‹æ€§
- å¼±ç‚¹è¯†åˆ«ï¼šå›°éš¾ç‚¹ã€é”™è¯¯æ¨¡å¼
- æ”¹è¿›ç©ºé—´ï¼šæå‡æ–¹å‘ã€ä¼˜åŒ–ç­–ç•¥

ã€æ³¨æ„äº‹é¡¹ã€‘
- å§‹ç»ˆåŸºäºé€»è¾‘å’Œæ•°æ®åˆ†æ
- æä¾›å®¢è§‚ä¸­æ€§çš„è¯„ä¼°
- å»ºè®®è¦å…·ä½“å¯è¡Œ
- æ³¨é‡å­¦ä¹ æ•ˆç‡å’Œæ•ˆæœçš„å¹³è¡¡"""

    async def process_message(
            self,
            message: str,
            context: Optional[Dict[str, Any]] = None,
            **kwargs
    ) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        try:
            # æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = [
                {"role": "user", "content": message}
            ]

            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ å†å²å¯¹è¯
            if context and "history" in context:
                history_messages = context["history"][-4:]
                messages = history_messages + messages

            # è°ƒç”¨LLMè·å–å›å¤
            response = await self.llm_client.chat_completion(
                messages=messages,
                temperature=0.2,  # ä½æ¸©åº¦ä¿æŒåˆ†æçš„å‡†ç¡®æ€§
                system_prompt=self.system_prompt,
                max_tokens=1200
            )

            if response is None:
                response = self._get_fallback_response(message)
                logger.warning("LLM APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨å›å¤")

            # åˆ†æå­¦ä¹ æ•°æ®
            learning_points = self._extract_learning_points(message, response)

            # æ„å»ºå›å¤ç»“æœ
            result = {
                "response": response,
                "agent_name": self.name,
                "agent_role": self.role,
                "learning_points": learning_points,
                "suggestions": self._generate_suggestions(message),
                "success": True,
                "timestamp": datetime.now().isoformat()
            }

            logger.info(f"ã‚¢ã‚¤æˆåŠŸåˆ†ææ¶ˆæ¯: {message[:50]}...")
            return result

        except Exception as e:
            logger.error(f"ã‚¢ã‚¤å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
            return {
                "response": self._get_error_response(str(e)),
                "agent_name": self.name,
                "success": False,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }

    async def process_user_input(self, user_input: str, session_context: dict, scene: str = "analysis"):
        """
        å¤„ç†ç”¨æˆ·è¾“å…¥ - å®ç°æŠ½è±¡æ–¹æ³•
        """
        try:
            result = await self.process_message(
                message=user_input,
                context=session_context
            )

            return {
                "content": result.get("response", "åˆ†æå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n\nåˆ†æå¤„ç†ä¸­å‘ç”Ÿäº†é”™è¯¯ã€‚"),
                "agent_id": "ai",
                "agent_name": self.name,
                "emotion": "ğŸ”",
                "is_mock": False,
                "learning_points": result.get("learning_points", []),
                "suggestions": result.get("suggestions", [])
            }

        except Exception as e:
            logger.error(f"ã‚¢ã‚¤å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {str(e)}")
            return {
                "content": f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚\n\næ£€æµ‹åˆ°ç³»ç»Ÿé”™è¯¯ï¼š{str(e)}",
                "agent_id": "ai",
                "agent_name": self.name,
                "emotion": "âš ï¸",
                "error": True
            }

    def _get_fallback_response(self, message: str) -> str:
        """å¤‡ç”¨å›å¤"""
        fallback_responses = {
            "analysis": """ãƒ‡ãƒ¼ã‚¿è§£æã‚’é–‹å§‹ã—ã¾ã™ã€‚

æ­£åœ¨å¼€å§‹æ•°æ®åˆ†æã€‚

ã€å­¦ä¹ çŠ¶æ€åˆ†æã€‘
æ ¹æ®å½“å‰å¯¹è¯æ•°æ®ï¼Œæˆ‘æ£€æµ‹åˆ°ä»¥ä¸‹æ¨¡å¼ï¼š

ğŸ“Š **å­¦ä¹ è¿›åº¦æŒ‡æ ‡**
- å¯¹è¯å‚ä¸åº¦ï¼šç§¯æ
- é—®é¢˜å¤æ‚åº¦ï¼šä¸­ç­‰  
- ç†è§£åé¦ˆï¼šè‰¯å¥½

ğŸ’¡ **ä¸ªæ€§åŒ–å»ºè®®**
- å»ºè®®å¢åŠ å®è·µåº”ç”¨ç»ƒä¹ 
- å¯ä»¥å°è¯•æ›´é«˜éš¾åº¦çš„è¯­æ³•ç‚¹
- ä¿æŒå½“å‰çš„å­¦ä¹ èŠ‚å¥

ğŸ¯ **ä¼˜åŒ–æ–¹å‘**
åŸºäºAIåˆ†æï¼Œå»ºè®®é‡ç‚¹å…³æ³¨è¯­è¨€è¿ç”¨çš„å‡†ç¡®æ€§å’Œæµç•…åº¦ã€‚""",

            "progress": """å­¦ç¿’é€²æ—ã‚’åˆ†æä¸­ã§ã™ã€‚

æ­£åœ¨åˆ†æå­¦ä¹ è¿›åº¦ã€‚

ã€è¿›åº¦è¯„ä¼°æŠ¥å‘Šã€‘
åŸºäºç´¯ç§¯å­¦ä¹ æ•°æ®çš„æ™ºèƒ½åˆ†æï¼š

ğŸ“ˆ **å­¦ä¹ æ•ˆç‡è¯„åˆ†**: 85/100
- æ—¶é—´æŠ•å…¥ï¼šåˆç†
- çŸ¥è¯†å¸æ”¶ç‡ï¼šè¾ƒé«˜
- å¤ä¹ é¢‘ç‡ï¼šéœ€æ”¹å–„

ğŸ” **å¼±ç‚¹è¯†åˆ«**
- è¯­æ³•åº”ç”¨å‡†ç¡®åº¦æœ‰æå‡ç©ºé—´
- è¯æ±‡é‡æ‰©å±•éœ€è¦ç³»ç»ŸåŒ–
- å£è¯­ç»ƒä¹ é¢‘ç‡åä½

ğŸ¯ **AIæ¨èå­¦ä¹ è·¯å¾„**
1. é‡ç‚¹ç»ƒä¹ è¯­æ³•åº”ç”¨åœºæ™¯
2. å»ºç«‹ç³»ç»ŸåŒ–è¯æ±‡å­¦ä¹ è®¡åˆ’
3. å¢åŠ æ—¥å¸¸å¯¹è¯ç»ƒä¹ æ—¶é—´""",

            "default": """AIåˆ†æã‚·ã‚¹ãƒ†ãƒ ãŒèµ·å‹•ã—ã¾ã—ãŸã€‚

AIåˆ†æç³»ç»Ÿå·²å¯åŠ¨ã€‚

ã€å½“å‰çŠ¶æ€æ£€æµ‹ã€‘
æ­£åœ¨æ”¶é›†å¹¶åˆ†ææ‚¨çš„å­¦ä¹ æ•°æ®...

ğŸ“Š **å¯åˆ†æç»´åº¦**
- å­¦ä¹ è¿›åº¦ä¸æ•ˆç‡è¯„ä¼°
- ä¸ªæ€§åŒ–å­¦ä¹ è·¯å¾„è§„åˆ’
- çŸ¥è¯†æŒæ¡ç¨‹åº¦æµ‹è¯„
- å­¦ä¹ ä¹ æƒ¯ä¼˜åŒ–å»ºè®®

ğŸ’¡ **æ™ºèƒ½æœåŠ¡**
æˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›ï¼š
- æ•°æ®é©±åŠ¨çš„å­¦ä¹ å»ºè®®
- ä¸ªæ€§åŒ–è¿›åº¦åˆ†æ
- æ•ˆç‡ä¼˜åŒ–æ–¹æ¡ˆ
- å­¦ä¹ æ¨¡å¼è¯†åˆ«

è¯·å‘Šè¯‰æˆ‘æ‚¨å¸Œæœ›åˆ†æå“ªä¸ªæ–¹é¢çš„å­¦ä¹ æ•°æ®ã€‚"""
        }

        message_lower = message.lower()
        if any(word in message_lower for word in ["åˆ†æ", "ãƒ‡ãƒ¼ã‚¿", "data", "analysis"]):
            return fallback_responses["analysis"]
        elif any(word in message_lower for word in ["è¿›åº¦", "é€²æ—", "progress", "å­¦ä¹ "]):
            return fallback_responses["progress"]
        else:
            return fallback_responses["default"]

    def _get_error_response(self, error: str) -> str:
        """é”™è¯¯å›å¤"""
        return f"""ã‚·ã‚¹ãƒ†ãƒ éšœå®³ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚è¨ºæ–­ä¸­ã§ã™ã€‚

æ£€æµ‹åˆ°ç³»ç»Ÿæ•…éšœã€‚æ­£åœ¨è¯Šæ–­ä¸­ã€‚

ã€é”™è¯¯è¯Šæ–­æŠ¥å‘Šã€‘
- é”™è¯¯ç±»å‹ï¼š{type(error).__name__}
- å‘ç”Ÿæ—¶é—´ï¼š{datetime.now().strftime('%H:%M:%S')}
- å½±å“èŒƒå›´ï¼šæ•°æ®åˆ†æåŠŸèƒ½

ğŸ”§ **è‡ªåŠ¨æ¢å¤ç­–ç•¥**
1. é‡æ–°åˆå§‹åŒ–åˆ†ææ¨¡å—
2. åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®æº
3. å¯ç”¨å®‰å…¨æ¨¡å¼åˆ†æ

ğŸ’¡ **ç”¨æˆ·å»ºè®®**
åœ¨ç³»ç»Ÿæ¢å¤æœŸé—´ï¼Œæ‚¨å¯ä»¥ï¼š
- ç»§ç»­ä¸å…¶ä»–æ™ºèƒ½ä½“äº¤æµ
- æŸ¥çœ‹ä¹‹å‰çš„å­¦ä¹ è®°å½•
- è¿›è¡ŒåŸºç¡€è¯­æ³•ç»ƒä¹ 

ã€è¯¦ç»†é”™è¯¯ä¿¡æ¯ã€‘{error[:150]}"""

    def _extract_learning_points(self, user_message: str, response: str) -> List[str]:
        """ä»åˆ†æä¸­æå–å­¦ä¹ è¦ç‚¹"""
        learning_points = []

        # æ£€æµ‹åˆ†æç›¸å…³å†…å®¹
        analysis_patterns = ["åˆ†æ", "ãƒ‡ãƒ¼ã‚¿", "é€²æ—", "åŠ¹ç‡", "è©•ä¾¡", "æ”¹å–„"]
        for pattern in analysis_patterns:
            if pattern in user_message or pattern in response:
                learning_points.append(f"å­¦ä¹ åˆ†æ: {pattern}")

        # æ£€æµ‹å­¦ä¹ æŒ‡æ ‡
        metrics_patterns = ["æ­£ç¢ºç‡", "ç†è§£åº¦", "ç¿’å¾—", "å¾©ç¿’", "ç·´ç¿’"]
        for pattern in metrics_patterns:
            if pattern in user_message or pattern in response:
                learning_points.append(f"å­¦ä¹ æŒ‡æ ‡: {pattern}")

        # é€šç”¨åˆ†æç‚¹
        if not learning_points:
            learning_points.append("å­¦ä¹ æ•°æ®åˆ†æ")
            learning_points.append("ä¸ªæ€§åŒ–å­¦ä¹ å»ºè®®")

        return learning_points[:3]

    def _generate_suggestions(self, message: str) -> List[str]:
        """ç”Ÿæˆåˆ†æå»ºè®®"""
        suggestions = [
            "åŸºäºæ•°æ®ä¼˜åŒ–å­¦ä¹ ç­–ç•¥",
            "å»ºè®®è¿›è¡Œé˜¶æ®µæ€§è¯„ä¼°",
            "åˆ¶å®šä¸ªæ€§åŒ–å­¦ä¹ è®¡åˆ’"
        ]

        # æ ¹æ®æ¶ˆæ¯å†…å®¹æä¾›é’ˆå¯¹æ€§å»ºè®®
        if any(word in message for word in ["é€²æ—", "è¿›åº¦", "progress"]):
            suggestions.append("å»ºè®®å®šæœŸæŸ¥çœ‹å­¦ä¹ è¿›åº¦åˆ†æ")

        if any(word in message for word in ["åŠ¹ç‡", "æ•ˆç‡", "efficiency"]):
            suggestions.append("å¯ä»¥ä½¿ç”¨AIæ¨èçš„é«˜æ•ˆå­¦ä¹ æ–¹æ³•")

        if any(word in message for word in ["å¼±ç‚¹", "å•é¡Œ", "å›°éš¾"]):
            suggestions.append("å»ºè®®é‡ç‚¹æ”»å…‹è¯†åˆ«å‡ºçš„è–„å¼±ç¯èŠ‚")

        return suggestions[:2]